## Techniques and challenges for benchmarking methods

### Definition of benchmarking

 Often, the goal in benchmarking is recovery of known cell types with
   processing of raw data, quantification, and clustering. The
   Adjusted Rand Index (ARI) or other metrics for partitions are
   used.

   One may also attempt to benchmark methods for their ability to
   discover known relationships between data modalities, e.g. gene
   regulatory relationships observed between chromatin accessibility
   and gene expression. However, this is made difficult by the fact
   that these relationships are not fully known at the single cell
   level.

### Strategies for benchmarking

Benchmarking multi-modal methods on typical multi-modal datasets is inherently difficult, as we rarely
  know the ground truth [@doi:10.1186/1471-2105-10-34]. Simulation is useful for having known truth, but it is difficult to
  simulate realistic covariance structure across features and across
  data modalities (**Figure**).

#### Creating benchmarking datasets
(**add examples from Google Doc**)
Benchmark datasets for single cell studies have largely centered around
  measuring sequencing depth and diversity of cell types derived from
  a single assay of interest (e.g. scRNAseq). A benchmark dataset
  serves a few purposes:

   - Provides ground truth for the intended effect of exposure in a
      proposed study design.
   - Provides validation for a data integration task for which a new
      computational method may be proposed.

  For multi-modal assays, while the intended effects can vary based on
  the leading biological questions, one may abstract out common data
  integration tasks such as co-embedding, mapping or correlation, and
  inferring causal relationships. We distinguish data integration from
  further downstream analyses that may occur on integrated samples
  such as differential analysis of both assays with regard to to a certain
  exposure.

  Both the intended effects and data integration task rely on study
  design that takes into account:

- Biological and technical variability via replicates, block
      design, and randomization.
- Power analysis for the intended effect or data integration
      task.
- Dependencies between modalities, for e.g. gene expression
      depending on gene regulatory element activity, requires that
      experiment design must also account for spatial and temporal
      elements in sampling for a given observation.

  As such, no universal benchmark data scheme may suit every
  combination of modality, and benchmark datasets may be established
  for commonly used combinations of modalities or technologies,
  towards specific data integration tasks.

#### Cross-validation within study
For example the [cross-validation analysis of the scNMT-seq dataset](https://github.com/Wancen/CV-MOFA)
  was performed using MOFA+ (**Figure**). Such an evaluation of methods using permutation or cross-validation
  has been performed previously, typically to optimize a tuning
  parameter or other aspects of model selection. Permutation has been
  used to create null datasets, either as demonstration that a method
  is not overfitting, or for tuning parameter selection, where the
  optimal parameter setting should produce an objective that is far
  from the null distribution
  [@doi:10.1186/1471-2105-4-59; @doi:10.2202/1544-6115.1470; @doi:10.1074/mcp.TIR118.001251].
  Cross-validation using folds or leave-one-out has likewise been used in many multi-modal method development papers
  [@doi:10.2202/1544-6115.1390; @doi:10.1016/j.jmva.2007.06.007; @doi:10.2202/1544-6115.1329; @doi:10.18637/jss.v023.i12; @doi:10.1142/S0218339009002831; @doi:10.1093/biostatistics/kxp008; @doi:10.2202/1544-6115.1406; @doi:10.1186/1471-2105-11-191; @doi:10.1186/1471-2164-13-160; @doi:10.1093/bioinformatics/bts476; @doi:10.1371/journal.pcbi.1005752; @doi:10.1093/bioinformatics/bty1054].

  A challenge with within study cross-validation is how to
  match dimensions of latent space across folds. Previous evaluations
  of multi-modal methods using cross-validation have focused on the
  top factor, swapping the sign of the project as needed to align
  the top latent factor across folds [@doi:10.1093/bib/bbz070].

  Finally, we note that for assessing clustering, a number of papers
  have suggested to resampling or data-splitting strategies to
  determine prediction strength
  [@doi:10.1073/pnas.161273698; @doi:10.1186/gb-2002-3-7-research0036; @doi:10.1198/106186005X59243; @doi:10.1093/jnci/djr545].
  For clustering of cells into putative cell types or cell states,
  such previously developed techniques could be applied in a
  multi-modal setting.

Cross-study validation would assess if relationships discovered in
  one dataset present in other datasets, potentially looking across
  single cell and bulk.

### Using hackathon studies as community benchmarking

*to fill here, could mention the dream challenge and link back to our own experience / learnings*




  **Figure** **A)** Silver standard: Svensson et al. (2017) Nat Methods; Wang et al. (2019) bioRxiv; Cole et al. (2019) Cell Systems; Zhang et al. (2017) bioRxiv;
Soneson et al. (2018) Nat Methods; Saelens et al. (2019) Nat Biotechnol; Gold standard control data: Tian et al. (2019) Nat Meth; Freytag et al. (2018) F1000Res;  Gold standard simulated data with the splatter R package appia et al. (2017) Genome Biol [credit: Matt Ritchie] **B)** scNMT-seq study: correlations with linear projections (MOFA+) evaluated with cross-validation [credit: Mike Love]. **DOI missing**

  <img src="images/Benchmark.png" alt="Silver and Gold standard benchmarking" height="300"/>
  <img src="images/scNMT_MOFA_CV.png" alt="Cross validation of MOFA+ on scNMT-seq" height="300"/>
