## Techniques and challenges for benchmarking methods

Visualizations and biological assessment of marker gene lists resulting from multi-omics analyses provide critical interpretation of
integrative analysis of high-throughput data, but additional quantitative metrics are also critical to delineate biologically-relevant
features from features that may derive from either computational or techical artifacts. 
Beyond interpretation, quantitative benchmarks are also essential to enable unbiased comparison between analytical methods. For example, the goal
of multi-platform single cell data analysis is often the recovery of known cell types through computational methods, where metric such as the
adjusted Rand Index (ARI) enable to directly assess the clustering results to already known cell types.
When cell types or biological features are not known *a priori*, benchmark methods can also be useed to discover known relationships between data modalities, e.g. gene regulatory relationships observed between chromatin accessibility and gene expression. However, as those relationships are not fully
known at the single cell level, it is essential that benchmark standards are set in place for validation (Figure {@fig:benchmark}A). 

  
![](images/Benchmark_mockup.png){#fig:benchmark}
Caption figure: **A** Silver standard: **Some explanation text here please about what we are showing**Svensson et al. (2017) Nat Methods; Wang et al. (2019) bioRxiv; Cole et al. (2019) Cell Systems; Zhang et al. (2017) bioRxiv; Soneson et al. (2018) Nat Methods; Saelens et al. (2019) Nat Biotechnol; Gold standard control data: Tian et al. (2019) Nat Meth; Freytag et al. (2018) F1000Res;  Gold standard simulated data with the splatter R package appia et al. (2017) Genome Biol (**Refs to be added proper**).
**B** scNMT-seq study: correlations with linear projections (MOFA+) evaluated with cross-validation.


### Strategies for benchmarking

Benchmarking multi-modal methods on typical multi-modal datasets is inherently difficult, as ground truth is rarely known. 
Known truth can be simulated by generating *in silico* high-throughput data, but in the context of data integration, the simulation of a realistic covariance structure across features and across data modalities is challenging [@doi:10.1093/bioinformatics/bty1054] and rely on an underlying statistical model that may introduce further biases into benchmark studies. Another strategy is to use cross-validation within a study, or conduct cross-study validation to assess whether solutions found by multi-modal methods generalize to held-out observations or held-out studies. The latter was somewhat attempted in the spatial proteomics cross-study hackathon, but where ground truth was unknown (@ref{sec:proteomics}).

<!--. Moreover, these simulations can inadvertently embed the same underlying assumptions as the computational methods employed for analysis, introducing further biases into benchmark studies. Therefore, high-throughput datasets with a known ground truth are also critical for multi-omics studies and robust testgrounds for future hackathon studies that were widely discussed throughout the workshop.-->

#### Creating benchmarking datasets

Benchmark datasets for single cell studies have largely focused on  measuring sequencing depth and diversity of cell types derived from a single assay of interest (e.g. scRNAseq). A benchmark dataset serves two main purposes: To provides ground truth for the intended effect of exposure in a
proposed study design, and to provide validation for a data integration task for which a new computational method may be proposed.

For multi-modal assays, while the intended effects can vary based on the leading biological questions, one may abstract out common data integration tasks such as co-embedding, mapping or correlation, and inferring causal relationships. We distinguish data integration from further downstream analyses that may occur on integrated samples such as differential analysis of both assays with regard to to a certain exposure.

Both the intended effects and data integration task rely on study design that takes into account the biological and technical variability via replicates, block
design, and randomization, the power analysis for the intended effect or data integration task, and the dependencies between modalities, for e.g. gene expression
depending on gene regulatory element activity, requires that experiment design must also account for spatial and temporal elements in sampling for a given observation.

As such, no universal benchmark data scheme may suit every combination of modality, and benchmark datasets may be established for commonly used combinations of modalities or technologies, towards specific data integration tasks.
  
  
#### Cross-validation within study

Cross-validation using folds or leave-one-out has been used for parameter to optimize a tuning parameter or other aspects of model selection in several multi-modal method development papers 
[@doi:10.2202/1544-6115.1390; @doi:10.1016/j.jmva.2007.06.007; @doi:10.2202/1544-6115.1329; @doi:10.18637/jss.v023.i12; @doi:10.1142/S0218339009002831; @doi:10.1093/biostatistics/kxp008; @doi:10.2202/1544-6115.1406; @doi:10.1186/1471-2105-11-191; @doi:10.1186/1471-2164-13-160; @doi:10.1093/bioinformatics/bts476; @doi:10.1371/journal.pcbi.1005752; @doi:10.1093/bioinformatics/bty1054].
Similarly, permutation creates null datasets, either as demonstration that a particular method is not overfitting, or for tuning parameter selection, where the optimal parameter setting should produce an objective that is far
from the null distribution [@doi:10.1186/1471-2105-4-59; @doi:10.2202/1544-6115.1470; @doi:10.1074/mcp.TIR118.001251].
Cross-validation is particularly useful as a quantitative assessment of a method's self-consistency, even though it cannot determine the accuracy of a
method in a completely unbiased way if we do not have access to an external test data set. The cross-validation analysis of the scNMT-seq dataset (third hackathon) using MOFA+ (Figure {@fig:benchmark}B) demonstrated that strong relationships found within pairs of modalities in the test data were likewise often found equally strong in held out cells.

The challenge in this context is to match dimensions of latent space across folds. Previous evaluations
of multi-modal methods have focused on the top factor, swapping the sign of the projection as needed to align
the top latent factor across folds [@doi:10.1093/bib/bbz070]. For clustering assessment, several studies have resampling or data-splitting strategies to
determine prediction strength
[@doi:10.1073/pnas.161273698; @doi:10.1186/gb-2002-3-7-research0036; @doi:10.1198/106186005X59243; @doi:10.1093/jnci/djr545]. These techniques coud be further extended in a multi-modal setting for clustering of cells into putative cell types or cell states.
Community-based benchmarking efforts in the area of multi-modal data analysis could follow the paradigm of the [DREAM Challenges](http://dreamchallenges.org/), where multi-modal training data provided and test samples held out to evaluate the method submissions from participating groups.

<!--
Mike: I will add the vignettes refs in a supp table and cross refer appropriately
[cross-validation analysis of the scNMT-seq dataset](https://mikelove.github.io/BIRSBIO2020.Benchmarking.CVmofa/)
was performed as part of the hackathon for this meeting using MOFA+ (Figure {@fig:benchmark}B).
-->

#### Cross-validation between studies

Our hackathons have emphasized on the need to access external studies for methods assessment and validation, where either ground truth would be known as a silver standard, or high-quality data are available through controlled experiments as gold standard (Figure {@fig:benchmark}A). To take advantage of all data and technologies available, cross-study validation could also extend to cross-platform, to assess whether if relationships discovered in one dataset is present in other datasets, looking across
single cell and bulk as was recently proposed in [@doi:10.1101/2020.03.09.984468].



