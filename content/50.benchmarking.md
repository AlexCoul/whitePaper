## Techniques and challenges for benchmarking methods

Visualizations and biological assessment of marker gene lists
resulting from multi-omics analyses provide critical interpretation of
integrative analysis of high-throughput data, but additional
quantitative metrics are necessary to delineate biologically-relevant
features from features that may derive from either computational or
technical artifacts. Beyond interpretation, quantitative benchmarks
are also essential to enable unbiased comparison between analytical
methods. For example, the goal of multi-platform single cell data
analysis is often the recovery of known cell types through
computational methods, where metrics such as the adjusted Rand Index
(ARI) enable direct assessment of the clustering results with respect
to known cell types.  When cell types or biological features are not
known *a priori*, benchmark methods can also be used to discover known
relationships between data modalities, for example *cis* gene
regulatory mechanisms observed between chromatin accessibility and
gene expression. However, as those relationships are not fully known
at the single cell level, it is essential that benchmark standards are
set in place for validation (Figure {@fig:benchmark}A).
  
![](images/Benchmark_mockup.png){#fig:benchmark}
Caption figure: **A** Silver standard: **Some explanation text here please about what we are showing**Svensson et al. (2017) Nat Methods; Wang et al. (2019) bioRxiv; Cole et al. (2019) Cell Systems; Zhang et al. (2017) bioRxiv; Soneson et al. (2018) Nat Methods; Saelens et al. (2019) Nat Biotechnol; Gold standard control data: Tian et al. (2019) Nat Meth; Freytag et al. (2018) F1000Res;  Gold standard simulated data with the splatter R package appia et al. (2017) Genome Biol (**Refs to be added proper**).
**B** scNMT-seq study: correlations with linear projections (MOFA+) evaluated with cross-validation.


### Strategies for benchmarking

Benchmarking multi-modal methods is inherently difficult, as ground
truth is rarely known. Known truth can be simulated by generating *in
silico* high-throughput data, but in the context of data integration,
the simulation of a realistic covariance structure across features and
across data modalities is challenging
[@doi:10.1093/bioinformatics/bty1054] and must rely on an underlying
generative model that may introduce further biases into benchmark
studies. Another strategy is to use cross-validation within a study,
or conduct cross-study validation to assess whether solutions found by
multi-modal methods generalize to held-out observations or held-out
studies. The latter was somewhat attempted in the spatial proteomics
cross-study hackathon, but where ground truth was unknown
(@ref{sec:proteomics}).

<!--. Moreover, these simulations can inadvertently embed the same underlying assumptions as the computational methods employed for analysis, introducing further biases into benchmark studies. Therefore, high-throughput datasets with a known ground truth are also critical for multi-omics studies and robust testgrounds for future hackathon studies that were widely discussed throughout the workshop.-->

#### Creating benchmarking datasets

Benchmark datasets for single cell studies have largely focused on  measuring sequencing depth and diversity of cell types derived from a single assay of interest (e.g. scRNAseq). A benchmark dataset serves two main purposes: To provides ground truth for the intended effect of exposure in a
proposed study design, and to provide validation for a data integration task for which a new computational method may be proposed.

For multi-modal assays, while the intended effects can vary based on the leading biological questions, one may abstract out common data integration tasks such as co-embedding, mapping or correlation, and inferring causal relationships. We distinguish data integration from further downstream analyses that may occur on integrated samples such as differential analysis of both assays with regard to to a certain exposure.

Both the intended effects and data integration task rely on study design that takes into account the biological and technical variability via replicates, block
design, and randomization, the power analysis for the intended effect or data integration task, and the dependencies between modalities, for e.g. gene expression
depending on gene regulatory element activity, requires that experiment design must also account for spatial and temporal elements in sampling for a given observation.

As such, no universal benchmark data scheme may suit every combination of modality, and benchmark datasets may be established for commonly used combinations of modalities or technologies, towards specific data integration tasks.
  
  
#### Cross-validation within study

As mentioned above, cross-validation within a representative
multi-modal study is one possible approach for quantitative assessment
for unbiased comparison of methods. We note that the approach of
cross-validation -- in which observations are split into folds or left
out individually for assessing model fit -- has been used often for
parameter tuning within methods, or for other aspects of model selection
[@doi:10.2202/1544-6115.1390; @doi:10.1016/j.jmva.2007.06.007;
@doi:10.2202/1544-6115.1329; @doi:10.18637/jss.v023.i12;
@doi:10.1142/S0218339009002831; @doi:10.1093/biostatistics/kxp008;
@doi:10.2202/1544-6115.1406; @doi:10.1186/1471-2105-11-191;
@doi:10.1186/1471-2164-13-160; @doi:10.1093/bioinformatics/bts476;
@doi:10.1371/journal.pcbi.1005752;
@doi:10.1093/bioinformatics/bty1054].  
Similarly, permutation has been used to create null datasets, either
as demonstration that a particular method is not overfitting, or for
parameter tuning, where the optimal parameter setting should result in
a model score that is far from the null distribution of model scores
[@doi:10.1186/1471-2105-4-59;
@doi:10.2202/1544-6115.1470; @doi:10.1074/mcp.TIR118.001251].
Cross-validation is particularly useful as a quantitative assessment
of a method's self-consistency, even though it cannot determine the
*accuracy* of a method in a completely unbiased way, if we do not have
access to an external test data set for further confirmation. 

A cross-validation analysis of the scNMT-seq dataset using MOFA+ was
performed as part of the third hackathon, and demonstrated that strong
relationships found among pairs of modalities in single cells used
for training the model were likewise often found equally strong
in held out cells (Figure {@fig:benchmark}B).  One challenge that this
hackathon attempt revealed was how to reliably match dimensions of
latent space across cross-validation folds. Previous evaluations of
multi-modal methods have focused only on the top ``latent factor''
[@doi:10.1093/bib/bbz070], however, as was seen in the evaluation of
MOFA+ on the scNMT-seq dataset, many latent factors can be reliably
discovered in held out cells in studies of complex biological
processes such as differentiation of embryonic cells.

For clustering assessment, several studies have used resampling or
data-splitting strategies to determine prediction strength
[@doi:10.1073/pnas.161273698;
@doi:10.1186/gb-2002-3-7-research0036; @doi:10.1198/106186005X59243;
@doi:10.1093/jnci/djr545]. These techniques could be further extended
in a multi-modal setting for clustering of cells into putative cell
types or cell states. Community-based benchmarking efforts in the
area of multi-modal data analysis could follow the paradigm of
the [DREAM Challenges](http://dreamchallenges.org/), with multi-modal
training data provided and test samples held out, in order to evaluate
the method submissions from participating groups.

<!--
Mike: I will add the vignettes refs in a supp table and cross refer appropriately
[cross-validation analysis of the scNMT-seq dataset](https://mikelove.github.io/BIRSBIO2020.Benchmarking.CVmofa/)
was performed as part of the hackathon for this meeting using MOFA+ (Figure {@fig:benchmark}B).
-->

#### Cross-validation between studies

Our benchmarking hackathons have emphasized the need to access
external studies for methods assessment and validation, where either
ground truth would be known as a silver standard, or high-quality data
are available through controlled experiments as gold standard (Figure
{@fig:benchmark}A). To take advantage of all data and technologies
available, cross-study validation could also extend to cross-platform,
to assess whether relationships discovered in one dataset are
present in other datasets, looking across single cell and bulk as was
recently proposed in [@doi:10.1101/2020.03.09.984468].

