## Techniques and challenges for benchmarking methods

### Definition of benchmarking

Visualizations and biological assessment of marker gene lists
resulting from multi-omics analyses provide critical interpretation of
integrative analysis of high-throughput data. Additional quantitative
metrics are essential to establish to delineate biologically-relevant
features from features resulting from either comptuational and
techical artifacts. Beyond interpretation, such quantitative
benchmarks are also essential to enable unbiased comparison between
the numerous analysis methods used for analysis. For example, the goal
of multi-platform single cell data analysis is often the recovery of
known cell types through computational methods. In this case, The
adjusted Rand Index (ARI) or other metrics for partitions can directly
compare known cell types with the results from processing of raw data,
quantification, and clustering. In cases where cell types or
biological features are unknown a priori, one may also attempt to
benchmark methods for their ability to discover known relationships
between data modalities, e.g. gene regulatory relationships observed
between chromatin accessibility and gene expression. However, this is
made difficult by the fact that these relationships are not fully
known at the single cell level which provides the very motivation for
the data generation and analysis. Cross-validation within
a study and cross-study validation can be used to assess whether
solutions found by multi-modal methods generalize to held-out
observations or held-out studies. 
   
![**A** Silver standard: Svensson et al. (2017) Nat Methods; Wang et al. (2019) bioRxiv; Cole et al. (2019) Cell Systems; Zhang et al. (2017) bioRxiv;
Soneson et al. (2018) Nat Methods; Saelens et al. (2019) Nat Biotechnol; Gold standard control data: Tian et al. (2019) Nat Meth; Freytag et al. (2018) F1000Res;  Gold standard simulated data with the splatter R package appia et al. (2017) Genome Biol [credit: Matt Ritchie] (**Refs to be added proper**).
**B** scNMT-seq study: correlations with linear projections (MOFA+) evaluated with cross-validation [credit: Wancen Mu and Michael Love].](images/Benchmark_mockup.png){#fig:benchmark}

### Strategies for benchmarking

Benchmarking multi-modal methods on typical multi-modal datasets is inherently difficult, as we rarely
  know the ground truth [@doi:10.1186/1471-2105-10-34]. Simulation methods that generate high-throughput data in silico are useful for having known truth, but it is difficult to
  simulate realistic covariance structure across features and across
  data modalities (Figure {@fig:benchmark}A). Moreover, these simulations can inadvertently embed the same underlying assumptions as the computational methods employed for analysis, introducing further biases into benchmark studies. Therefore, high-throughput datasets with a known ground truth are also critical for multi-omics studies and robust testgrounds for future hackathon studies that were widely discussed throughout the workshop.
  
#### Cross-validation within study

One strategy for quantitative assessment of multi-model methods is
cross-validation (CV). While CV cannot determine the accuracy of a
method, it can be used to assess a method's self-consistency.
For example a 
[cross-validation analysis of the scNMT-seq dataset](https://mikelove.github.io/BIRSBIO2020.Benchmarking.CVmofa/)
was performed as part of the hackathon for this meeting using MOFA+ (Figure {@fig:benchmark}B).
Such community-based benchmarking efforts in the area of multi-modal 
data analysis could follow the paradigm of the [DREAM Challenges](http://dreamchallenges.org/),
with multi-modal training data provided and test samples held out in order
to evaluate the method submissions from participating groups.

Evaluation of methods using permutation or cross-validation 
has been performed previously, typically to optimize a tuning
parameter or other aspects of model selection. Permutation has been
used to create null datasets, either as demonstration that a method
is not overfitting, or for tuning parameter selection, where the
optimal parameter setting should produce an objective that is far
from the null distribution
[@doi:10.1186/1471-2105-4-59; @doi:10.2202/1544-6115.1470; @doi:10.1074/mcp.TIR118.001251].
Cross-validation using folds or leave-one-out has likewise been used
in many multi-modal method development papers 
[@doi:10.2202/1544-6115.1390; @doi:10.1016/j.jmva.2007.06.007; @doi:10.2202/1544-6115.1329; @doi:10.18637/jss.v023.i12; @doi:10.1142/S0218339009002831; @doi:10.1093/biostatistics/kxp008; @doi:10.2202/1544-6115.1406; @doi:10.1186/1471-2105-11-191; @doi:10.1186/1471-2164-13-160; @doi:10.1093/bioinformatics/bts476; @doi:10.1371/journal.pcbi.1005752; @doi:10.1093/bioinformatics/bty1054].

A challenge with within study cross-validation is how to
match dimensions of latent space across folds. Previous evaluations
of multi-modal methods using cross-validation have focused on the
top factor, swapping the sign of the project as needed to align
the top latent factor across folds [@doi:10.1093/bib/bbz070].

Finally, we note that for assessing clustering, a number of papers
have suggested to resampling or data-splitting strategies to
determine prediction strength
[@doi:10.1073/pnas.161273698; @doi:10.1186/gb-2002-3-7-research0036; @doi:10.1198/106186005X59243; @doi:10.1093/jnci/djr545].
For clustering of cells into putative cell types or cell states,
such previously developed techniques could be applied in a
multi-modal setting.

Cross-study validation would assess if relationships discovered in
one dataset present in other datasets, potentially looking across
single cell and bulk.

#### Creating benchmarking datasets
(**add examples from Google Doc**)

Benchmark datasets for single cell studies have largely centered around
  measuring sequencing depth and diversity of cell types derived from
  a single assay of interest (e.g. scRNAseq). A benchmark dataset
  serves a few purposes:

   - Provides ground truth for the intended effect of exposure in a
      proposed study design.
   - Provides validation for a data integration task for which a new
      computational method may be proposed.

  For multi-modal assays, while the intended effects can vary based on
  the leading biological questions, one may abstract out common data
  integration tasks such as co-embedding, mapping or correlation, and
  inferring causal relationships. We distinguish data integration from
  further downstream analyses that may occur on integrated samples
  such as differential analysis of both assays with regard to to a certain
  exposure.

  Both the intended effects and data integration task rely on study
  design that takes into account:

- Biological and technical variability via replicates, block
      design, and randomization.
- Power analysis for the intended effect or data integration
      task.
- Dependencies between modalities, for e.g. gene expression
      depending on gene regulatory element activity, requires that
      experiment design must also account for spatial and temporal
      elements in sampling for a given observation.

  As such, no universal benchmark data scheme may suit every
  combination of modality, and benchmark datasets may be established
  for commonly used combinations of modalities or technologies,
  towards specific data integration tasks.
