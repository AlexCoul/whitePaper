## Techniques and challenges for benchmarking methods

### We must first define what we are benchmarking

 * Often the goal in benchmarking is recovery of known cell types with
   processing of raw data, quantification, and clustering. The
   Adjusted Rand Index (ARI) or other metrics for partitions are
   used.
 * One may also attempt to benchmark methods for their ability to
   discover known relationships between data modalities, e.g. gene
   regulatory relationships observed between chromatin accessibility
   and gene expression. However, this is made difficult by the fact
   that these relationships are not fully known at the single cell
   level.

### Strategies for benchmarking

* Simulation is useful for having known truth, but it is difficult to
  simulate realistic covariance structure across features and across
  data modalities.
* Benchmarking datasets (add examples from Google Doc). Benchmark
  datasets for single cell studies have largely centered around
  measuring sequencing depth and diversity of cell types derived from
  a single assay of interest (e.g. scRNAseq). A benchmark dataset
  serves a few purposes:
    - Provides ground truth for the intended effect of exposure in a
      proposed study design.
    - Provides validation for a data integration task for which a new
      computational method may be proposed.

  For multi-modal assays, while the intended effects can vary based on
  the leading biological questions, one may abstract out common data
  integration tasks such as co-embedding, mapping or correlation, and
  inferring causal relationships. We distinguish data integration from
  further downstream analyses that may occur on integrated samples
  such as differential analysis of both assays with regard to to a certain
  exposure.

  Both the intended effects and data integration task rely on study
  design that takes into account:
    - Biological and technical variability via replicates, block
      design, and randomization.
	- Power analysis for the intended effect or data integration
      task.
	- Dependencies between modalities, for e.g. gene expression
      depending on gene regulatory element activity, requires that
      experiment design must also account for spatial and temporal
      elements in sampling for a given observation.

  As such, no universal benchmark data scheme may suit every
  combination of modality, and benchmark datasets may be established
  for commonly used combinations of modalities or technologies,
  towards specific data integration tasks.

* Cross-validation within study can be performed. For example the
  following 
  [cross-validation analysis of the scNMT-seq dataset](https://github.com/Wancen/CV-MOFA)
  was performed using MOFA+



  A challenge with within study cross-validation is how to
  match dimensions of latent space across folds. (add examples from
  Google Doc of papers that have performed either permutation or
  cross-validation to assess model performance)
* Cross-study validation would assess if relationships discovered in
  one dataset present in other datasets, potentially looking across
  single cell and bulk.
  
  **Figure** **A)** Silver standard: Svensson et al. (2017) Nat Methods; Wang et al. (2019) bioRxiv; Cole et al. (2019) Cell Systems; Zhang et al. (2017) bioRxiv; 
Soneson et al. (2018) Nat Methods; Saelens et al. (2019) Nat Biotechnol; Gold standard control data: Tian et al. (2019) Nat Meth; Freytag et al. (2018) F1000Res;  Gold standard simulated data with the splatter R package appia et al. (2017) Genome Biol [credit: Matt Ritchie] **B)** scNMT-seq study: correlations with linear projections (Sparse mCCA and MOFA(+)) evaluated with cross-validation.

  <img src="images/Benchmark.png" alt="Silver and Gold standard benchmarking" height="300"/>
  <img src="images/scNMT_MOFA_CV.png" alt="Cross validation of MOFA+ on scNMT-seq" height="300"/>
    
    
    
