## Software strategies to enable analyses of multimodal single cell experiments

The application of novel comptuational frameworks for multi-omics analysis is also limited by software tools to enable their broad dissemination. In this section, we reflect on the challenges we have faced when analyzing this series of hackathons whilst using analytic software for visualization and inference in multimodal single-cell experiments. Our discussion is necessarily limited in scope, but
we provide pointers to concrete details when relevant.

### Basic aims

We take it for granted that __openness__ is a _sine qua non_ for computational tooling in this
area.  All components need to be accessible for full vetting by the community, so licensing
in Creative Commons, Artistic, or GNU frameworks is expected.  We also aim for a __coordinated approach__,
so that duplication of effort between groups working on similar problems can be avoided.  Finally,
we seek solutions that are __efficient__, avoid __lock-in__, and lead to reproducible analyses.
Real-time improvements to the tool-set should
be feasible, respecting needs for stability, reliability, and continuity of access to evolving components.

These objectives are fluid and open to interpretation.  Community engagement and communication are
important to achieving desired goals in this domain.

### Key questions

* How should multimodal single cell data be managed for interactive and batch analyses?
* What methods will help software developers create scalable solutions for multimodal single cell analysis?
* How can we ensure that visualization methods that are central to multimodal single cell analysis
are usable by researchers with visual impairments?

These questions will ultimately be answered through the creation of a __software ecosystem__ [@doi:10.1126/science.aaf6162; @https://chaoss.github.io/grimoirelab/; @http://ceur-ws.org/Vol-987/3.pdf].
As an example of an ecosystem of broad scope, we cite bioconductor.org [@https://bioconductor.org].  This project
produces code in R for __data representation and data services__ for many data modalities used
in genome-scale experimentation.  Bioconductor's resources for achieving __scalability__ [@doi:10.1184/R1/6575879.v1] include
tools for analyzing massive data resources with tunable RAM footprints [@10.18129/B9.bioc.DelayedArray; @10.18129/B9.bioc.rhdf5], and tooling for
supporting fault-tolerant parallel distributed computing in various cluster and cloud contexts [@https://bioconductor.org/packages/BiocParallel].
Finally, Bioconductor supports _developers_ who seek to build broad user bases by providing
multiplatform/multistream __continuous integration/continuous delivery__ of contributed packages [@https://bioconductor.org/checkResults/],
and _users_ with different skill sets by articulating standards for documentation, and testing, and
by hosting community forums and workshops [@https://bioconductor.org/support/].

### Data management strategies

__A ready-to-use integrative data class with `multiAssayExperiment`.__
The Bioconductor S4 class implementing an abstract data type called `multiAssayExperiment` is highly relevant
for multimodal single-cell experiments as each mode is characterized by a different collection of features on possibly non-overlapping collections
of samples [@doi:10.1158/0008-5472.CAN-17-0344].

The Metadata on features is bound directly into the class instance.  For example,
genes and transcripts can be enumerated using Ensembl [@doi:10.1093/nar/gkz966] catalog identifiers, represented as
`GRanges` instances [@doi:10.1371/journal.pcbi.1003118]; regions of accessibility
from, e.g., ATAC-seq experiments, may be defined using genomic coordinates in a clearly specified reference build.  Metadata on
samples includes all relevant information on experimental conditions such as treatment,
protocol, and date of technical processing.  Figure {@fig:spatialExpt} shows how this
class was used to amalgamate and annotate results of a multimodal dataset consisting of seqFISH and scRNA-seq experimental data.
To combine these two different experiments, the seqFISH data were stored into the SpatialExperiment S4 class object, while the scRNA-seq data were stored into a SingleCellExperiment Bioconductor class object [@doi:10.18129/B9.bioc.SingleCellExperiment]. 
Then, these objects were easily stored into a MultiAssayExperiment class object and released with the SingleCellMultiModal Bioconductor package [@doi:10.18129/B9.bioc.SingleCellMultiModal]. 

![Panel A: Combination of seqFISH-based SpatialExperiment
and SingleCellExperiment instances into a MultiAssayExperiment.  Panel B: details of the SpatialExperiment class design.](images/MSA_SpE.png){#fig:spatialExpt width="50%"}

The `multiAssayExperiment` class includes
   1) Assay slots containing variables or features from multiple modalities (e.g. gene expression units from scRNA-seq and protein units in sc-proteomics), either from the same cells or distinct cells from the same or distinct starting samples or biological specimen of origin. In some cases, the feature may be multidimensional (e.g. spatial coordinates, locations of eQTLs).
   2) Metadata for sample of origin for the individual cells, e.g. study, center, phenotype, perturbation.
   3) A map between the different assays to enable analysis.

Some of our contributors (Al Abadi, Patheepa Jeganathan) used the `multiAssayExperiment` class to integrate the multi-modal single cell data (scNMT-seq, sc Proteomics, **pointers to vignettes**), allowing for easier preprocessing, transformation, extraction of spatial information from raster objects, addition of cell information and visualization **Al, details please if needed**.

<!--In our hackathon context, we considered multi-assay measurements from the same cell (e.g. scNMT-seq) or integration of multi-assay measurements from  (seqFish, scProteomics).
-->

While RNA-seq has well-defined units and IDs (e.g., transcript names), other assays may be summarized at different genomic scales, e.g., gene promoters, exons, introns, or gene bodies. The `GenomicRanges` package [@doi:10.18129/B9.bioc.GenomicRanges] may be used to compute summaries at different scales and overlap between signal (e.g., ATAC-seq peaks) and genomic annotation. Further, the observations of different modalities may not be directly comparable: for instance, gene expression may be measured from individual cells in single-cell RNA-seq but spatial transcriptomics may have a finer (sub-cellular) or coarser (multi-cellular) resolution. Methods such as SPOTlight [@doi:10.1101/2020.06.03.131334] may be used to deconvolute multi-cellular spots signal. Finally, in the absence of universal standards, the metadata available may vary from analysis to analysis. A potential solution is to define the minimum set of metadata variables necessary for each assay, and for pairs of assays to be comparable for common analyses.

### Scalability strategies

In addition to standardize data infrastructures that allows for scalability of _storing_ and _access_ of large datasets [@doi:10.18129/B9.bioc.DelayedArray; @doi:10.18129/B9.bioc.rhdf5], new strategies are emerging that allow for scalable _algorithms_ that allow for data to be stored in memory or on disk such as unsupervised clustering of cell types in either R [@doi:10.1101/2020.05.27.119438; @doi:10.18129/B9.bioc.mbkmeans] or Python [@doi:10.1186/s13059-017-1382-0]. While these strategies were originally developed in application of single-cell RNA-sequencing analyses, these scalable algorithms are applicable to multimodal single-cell experiments.


### Data access for the hackathons and long term software strategy for multimodal single-cell experiments

Reproducibility and transparency are crucial aspect in hackathons, starting with data availability. Providing data to the community is a long standing issue (e.g. SAGEBionetwork **complete Elana**). In our context, input data across all hackathons were pre-processed with steps documented, and the data were added into the ExperimentHub package `SingleCellMultimodal` [doi:10.18129/B9.bioc.SingleCellMultiModal], developed to incorporate multiple multi modal datasets in MultiAssaiExperiment format so that they can be used for further computational developments by others. At the moment, it includes the hackathons scNMT-seq and the seqFISH+scRNA-seq (Fig. {@fig:spatialExpt}) datasets, easily stored because of the samples overlapping between multiple modalities, while we are working on the integration of the scProteomics dataset which has no overlap between samples.

The vignettes of all contributors were included into containers **Al, details please** using the `ExperimentHub` package as a dependency. In these containers, we used consistent assay access methods to powerful implementation strategies (possibly through methods inheritance. e.g. from `SummarizedExperiment` **Al please amend**). Such setting will in the long-term facilitate Continuous Integration (CI) of the analyses and preprocessing changes and Continuous Delivery (CD)
of the analysis reports. The CI/CD workflow may also be automated on a hosted server and containerized
reports can be generated for enhanced efficiency and portability.


### Reducing barriers to interpretable visualizations

Color is a powerful data visualization tool that helps representing the different dimensions of our increasingly complex and rich scientific data.
Color vision deficiencies affect a substantial portion of the population[@https://tinyurl.com/y4emdyvr] and leads to difficulties in perceiving patterns (the basis for the Ishihara's color vision tests) in multi-colored figures. In some cases, the perceived patterns such as heat maps and reduced dimension plots can differ between individuals with normal and color deficient vision.

One strategy to present scientific information accessible information to all readers is to include colorblind friendly visualizations [@https://doi.org/10.1038/nmeth.1618; @doi:10.1038/nmeth0810-573] as a default setting, using palettes such as form the R packages viridis [@https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html] and dittoSeq [@https://github.com/dtm2451/dittoSeq], whilst limiting to a number of 10 colors. Additional visual cues to differentiate regions (hatched areas) or cells (point shapes) can also reduce the dependence on colors. The inclusion an "accessibility caption" accompanying figures which "guide" the reader's perception of the images would greatly benefit broader data accessibility.


<!-- Overall, a broader discussion regarding the accessibility of our figures that is not just limited to color vision deficiencies would be greatly beneficial towards improving data accessibility.
Perhaps one tool to address broader accessibility could be
[US Government tools for accessibility](https://accessibility.18f.gov/tools/)
-->

### Details of working components -- trimmed

you can interact with underlying data at [google sheet](https://docs.google.com/spreadsheets/d/1tSUQ9iDKqq72TB9G3Cx1evg2sekS-ytx5wRXDv6vxfg/edit?usp=sharing)

|Type|Brief name (link)|Description|
|----|-----------------|-----------|
|Matlab package|[CytoMAP](https://gitlab.com/gernerlab/cytomap)|CytoMAP: A Spatial Analysis Toolbox Reveals Features of Myeloid Cell Organization in Lymphoid Tissues|
|Matlab package|[histoCAT](https://github.com/BodenmillerGroup/histoCAT)|histoCAT: analysis of cell phenotypes and interactions in multiplex image cytometry data|
|Python library|[PyTorch](https://pytorch.org)|General framework for deep learning|
|Python package|[SpaCell](https://github.com/BiomedicalMachineLearning/SpaCell)|SpaCell: integrating tissue morphology and spatial gene expression to predict disease cells|
|Python package|[Scanpy](https://github.com/theislab/scanpy)|Python package for single cell analysis|
|R data class|[MultiAssayExperiment](https://bioconductor.org/packages/MultiAssayExperiment)|unify multiple experiments|
|R data class|[SpatialExperiment](https://github.com/drighelli/SpatialExperiment)|SpatialExperiment: a collection of S4 classes for Spatial Data|
|R package|[Giotto](https://github.com/RubD/Giotto)|Spatial transcriptomics|
|R package|[cytomapper](https://github.com/BodenmillerGroup/cytomapper)|cytomapper: Visualization of highly multiplexed imaging cytometry data in R|
|R package|[Spaniel](https://github.com/RachelQueen1/Spaniel/)|Spaniel: analysis and interactive sharing of Spatial Transcriptomics data|
|R package|[Seurat](https://github.com/satijalab/seurat)|R toolkit for single cell genomics|
|R package|[SpatialLIBD](https://github.com/LieberInstitute/spatialLIBD)|Transcriptome-scale spatial gene expression in the human dorsolateral prefrontal cortex|
|R package|[Cardinal](https://cardinalmsi.org/)|Cardinal: an R package for statistical analysis of mass spectrometry-based imaging experiments|
|R package|[CoGAPS](https://github.com/FertigLab/CoGAPS)|scCoGAPS learns biologically meaningful latent spaces from sparse scRNA-Seq data|
|R package|[projectR](https://github.com/genesofeve/projectR)|ProjectR is a transfer learning framework to rapidly explore latent spaces across independent datasets|
|R package|[SingleCellMultiModal](https://github.com/waldronlab/SingleCellMultiModal)|Serves multiple datasets obtained from GEO and other sources and represents them as MultiAssayExperiment objects|
|R scripts|[SpatialAnalysis](https://github.com/drighelli/SpatialAnalysis)|Scripts for SpatialExperiment usage|
|Self-contained GUI|[ST viewer](https://github.com/jfnavarro/st_viewer)|ST viewer: a tool for analysis and visualization of spatial transcriptomics datasets|
|Shiny app|[Dynverse](https://zouter.shinyapps.io/server/)|A comparison of single-cell trajectory inference methods: towards more accurate and robust tools|
|R package|[mixOmics](https://github.com/mixOmicsTeam/mixOmics)|R toolkit for multivariate analysis of multi-modal data|
|Python package|[totalVI](https://github.com/YosefLab/scVI)|A variational autoencoder (deep learning model) to integrate RNA and protein data from CITE-seq experiments|
|Python web application||[ImJoy](https://imjoy.io/#/)|Deep learning for image analysis|
|Python package|[napari](https://github.com/napari/napari)|Interactive big multi-dimensional 3D image viewer|
|Software|[QuPath](https://qupath.github.io/)|Multiplex whole slide image analysis|
|Python package|[Cytokit](https://github.com/hammerlab/cytokit)|Multiplex whole slide image analysis|
|Python package|[cmIF](https://gitlab.com/engje/cmif)|Multiplex whole slide image analysis|
|Software|[Facetto](https://github.com/kruegert/facetto)|Multiplex whole slide image analysis, not available yet|
|Software, Python based|[CellProfiler](https://cellprofiler.org/)|Image analysis|
