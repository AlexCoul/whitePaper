## Software strategies to enable analyses of multimodal single cell experiments

In this chapter we review the situation of scientists who
create and use analytic software for visualization and inference in multimodal
single-cell experiments.  Our discussion is necessarily limited in scope, but
we provide pointers to concrete details when relevant.

### Basic aims

We take it for granted that __openness__ is a _sine qua non_ for computational tooling in this
area.  All components need to be accessible for full vetting by the community, so licensing
in Creative Commons, Artistic, or GNU frameworks is expected.  We also aim for a __coordinated approach__,
so that duplication of effort between groups working on similar problems can be avoided.  Finally,
we seek solutions that are __efficient__ and avoid __lock-in__.  Real-time improvements to the tool-set should
be feasible, respecting needs for stability, reliability, and continuity of access to evolving components.

These objectives are fluid and open to interpretation.  Community engagement and communication are
important to achieving desired goals in this domain.

### Key questions

* How should multimodal single cell data be managed for interactive and batch analyses?
* What methods will help software developers create scalable solutions for multimodal single cell analysis?
* How can we ensure that visualization methods that are central to multimodal single cell analysis
are usable by researchers with visual impairments?

These questions will ultimately be answered through the creation of a __software ecosystem__.
As an example of an ecosystem of broad scope, we cite bioconductor.org.  This project
produces code in R for __data representation and data services__ for many data modalities used
in genome-scale experimentation.  Bioconductor's resources for achieving __scalability__[@doi:10.1184/R1/6575879.v1] include
tools for analyzing massive data resources with tunable RAM footprints, and tooling for
supporting fault-tolerant parallel distributed computing in various cluster and cloud contexts.
Finally, Bioconductor supports _developers_ who seek to build broad user bases by providing
multiplatform/multistream __continuous integration/continuous delivery__ of contributed packages,
and _users_ with different skill sets by articulating standards for documentation, and testing, and
by hosting community forums and workshops.

### Data management strategies

__A ready-to-use integrative data class.__ The Waldron group at City University of New York has defined a Bioconductor
class implementing an abstract data type called "multiassay experiment".  This is relevant
for multimodal single-cell experiments as each mode will
be characterized by a different collection of features on possibly non-overlapping collections
of samples.  Metadata on features is bound directly into the class instance.  For example,
genes and transcripts are enumerated using Ensembl catalog identifiers, represented as
GRanges instances; regions of accessibility
are from ATAC-seq are defined using genomic coordinates in a clearly specified reference build.  Metadata on
samples includes all relevant information on experimental conditions such as treatment,
protocol, and date of technical processing.  Figure {@fig:spatialExpt} shows how this
class can incorporate results of a seqFISH experiment.

![Left: Combination of seqFISH-based SpatialExperiment
and SingleCellExperiment instances into a MultiAssayExperiment.  Right: details of the SpatialExperiment class design.](images/fusedDataStructures.png){#fig:spatialExpt width="85%"}

Multi-modal single cell data may consist of multi-assay measurements from the same cell (e.g. CITE-seq, sci-CAR) or integration of multi-assay measurements from distinct cells from the same or distinct starting samples. A sample here refers to the biological specimen of origin (tissue A from individual X).
The MultiAssayExperiment class includes
   1) Assay slots containing variables or features from multiple modalities (e.g. gene expression units from scRNA-seq and protein units from sc-proteomics). In some cases, the feature may be multidimensional (e.g. spatial coordinates, locations of eQTLs).
   2) Metadata for sample of origin for the individual cells, e.g. study, center, phenotype, perturbation.
   3) A map between the different assays to enable analysis

Of note:
      - The observations of different modalities may not be directly comparable (e.g. RNA may be measured from individual cells but spatial transcriptomics may cover a few cells in the matched area).  
      - In the absence of universal standards, the metadata may vary from analysis to analysis.
      - It is crucial that data containers use consistent assay access methods (possibly through methods inheritance. e.g. from `SummarizedExperiment`). This will ensure less redundancy in development process and allow powerful implementation strategies.


* Serializations and data access methods for
    * spatial transcriptomics
    * scNMT-seq ...




### Reducing barriers to interpretable visualizations
Color is a powerful data visualization tool that helps representing the different dimensions of our increasingly complex and rich scientific data.
Color vision deficiencies affect a substantial portion of the population.
Therefore,  it is desirable to aim towards presenting scientific information in a manner that is as accessible as possible for all readers. 
Color vision deficiency leads to difficulties in perceiving patterns (the basis for the Ishihara's color vision tests) in multi-colored figures.
In rare cases, the perceived patterns; e.g. in heatmaps and reduced dimension plots, can differ between individuals with normal and color deficient vision.

One strategy to address these issues is to include colorblind friendly visualizations [@https://doi.org/10.1038/nmeth.1618] as a default setting in our visualizations.
Several colorblind-friendly palettes exist (e.g., see R packages [viridis](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html) and [dittoSeq](https://github.com/dtm2451/dittoSeq)) and can be integrated into data presentation as the default option.
Even with these palettes in place, it is desirable to limit the number (about 8-10 at a maximum) of colors in visualizations.
To reduce the dependence on colors, one solution would be to include additional visual cues to differentiate regions (hatched areas) or cells (point shapes).
Overall, a broader discussion regarding the accessibility of our figures that is not just limited to color vision deficiencies would be greatly beneficial towards improving data accessibility.
Perhaps one tool to address broader accessibility could be the inclusion an "accessibility caption" accompanying figures which "guide" the reader's perception of the images.


[Reference 1: Color Coding](https://www.nature.com/articles/nmeth0810-573)

[Reference 2: Points of View: Color Blindness](https://www.nature.com/articles/nmeth.1618)

[Viridis Color Palettes](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html)

[An overview of the issues with impaired color perception](https://www.vwvj.be/sites/default/files/zien/zien_-_uit_voorbije_vorming/kleurzinonderzoekextratips.pdf)

[US Government tools for accessibility](https://accessibility.18f.gov/tools/)

### Details of working components -- trimmed

you can interact with underlying data at [google sheet](https://docs.google.com/spreadsheets/d/1tSUQ9iDKqq72TB9G3Cx1evg2sekS-ytx5wRXDv6vxfg/edit?usp=sharing)

|Type|Brief name (link)|Description|
|----|-----------------|-----------|
|Matlab package|[CytoMAP](https://gitlab.com/gernerlab/cytomap)|CytoMAP: A Spatial Analysis Toolbox Reveals Features of Myeloid Cell Organization in Lymphoid Tissues|
|Matlab package|[histoCAT](https://github.com/BodenmillerGroup/histoCAT)|histoCAT: analysis of cell phenotypes and interactions in multiplex image cytometry data|
|Python library|[PyTorch](https://pytorch.org)|General framework for deep learning|
|Python package|[SpaCell](https://github.com/BiomedicalMachineLearning/SpaCell)|SpaCell: integrating tissue morphology and spatial gene expression to predict disease cells|
|Python package|[Scanpy](https://github.com/theislab/scanpy)|Python package for single cell analysis|
|R data class|[MultiAssayExperiment](https://bioconductor.org/packages/MultiAssayExperiment)|unify multiple experiments|
|R data class|[SpatialExperiment](https://github.com/drighelli/SpatialExperiment)|SpatialExperiment: a collection of S4 classes for Spatial Data|
|R package|[Giotto](https://github.com/RubD/Giotto)|Spatial transcriptomics|
|R package|[cytomapper](https://github.com/BodenmillerGroup/cytomapper)|cytomapper: Visualization of highly multiplexed imaging cytometry data in R|
|R package|[Spaniel](https://github.com/RachelQueen1/Spaniel/)|Spaniel: analysis and interactive sharing of Spatial Transcriptomics data|
|R package|[Seurat](https://github.com/satijalab/seurat)|R toolkit for single cell genomics|
|R package|[SpatialLIBD](https://github.com/LieberInstitute/spatialLIBD)|Transcriptome-scale spatial gene expression in the human dorsolateral prefrontal cortex|
|R package|[Cardinal](https://cardinalmsi.org/)|Cardinal: an R package for statistical analysis of mass spectrometry-based imaging experiments|
|R package|[CoGAPS](https://github.com/FertigLab/CoGAPS)|scCoGAPS learns biologically meaningful latent spaces from sparse scRNA-Seq data|
|R package|[projectR](https://github.com/genesofeve/projectR)|ProjectR is a transfer learning framework to rapidly explore latent spaces across independent datasets|
|R package|[SingleCellMultiModal](https://github.com/waldronlab/SingleCellMultiModal)|Serves multiple datasets obtained from GEO and other sources and represents them as MultiAssayExperiment objects|
|R scripts|[SpatialAnalysis](https://github.com/drighelli/SpatialAnalysis)|Scripts for SpatialExperiment usage|
|Self-contained GUI|[ST viewer](https://github.com/jfnavarro/st_viewer)|ST viewer: a tool for analysis and visualization of spatial transcriptomics datasets|
|Shiny app|[Dynverse](https://zouter.shinyapps.io/server/)|A comparison of single-cell trajectory inference methods: towards more accurate and robust tools|
|R package|[mixOmics](https://github.com/mixOmicsTeam/mixOmics)|R toolkit for multivariate analysis of multi-modal data|
|Python package|[totalVI](https://github.com/YosefLab/scVI)|A variational autoencoder (deep learning model) to integrate RNA and protein data from CITE-seq experiments|
|Python web application||[ImJoy](https://imjoy.io/#/)|Deep learning for image analysis|
|Python package|[napari](https://github.com/napari/napari)|Interactive big multi-dimensional 3D image viewer|
|Software|[QuPath](https://qupath.github.io/)|Multiplex whole slide image analysis|
|Python package|[Cytokit](https://github.com/hammerlab/cytokit)|Multiplex whole slide image analysis|
|Python package|[cmIF](https://gitlab.com/engje/cmif)|Multiplex whole slide image analysis|
|Software|[Facetto](https://github.com/kruegert/facetto)|Multiplex whole slide image analysis, not available yet|
|Software, Python based|[CellProfiler](https://cellprofiler.org/)|Image analysis|

### Scalability strategies

### Connecting the hackathon activities to the longer-term software strategy for multimodal single-cell experiments

The contributed challenge analyses as well as the used datasets may be collated into software packages
which include their specific dependencies. In such a setting, the analysis packages can simply use the
corresponding dataset package(s) as a dependency. Additionally, if the challenge datasets have undergone
preprocessing it is essential that all the preprocessing steps are outlined and made available in the
dataset packages. This will facilitate transparency and analysis reproducibility, as well as allow
Continuous Integration (CI) of the analyses and preprocessing changes and Continuous Delivery (CD)
of the analysis reports. The CI/CD workflow may also be automated on a hosted server and containerized
reports can be generated for enhanced efficiency and portability, respectively.






